{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06325279-877a-4b6a-b748-7d89373fbe9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb1c55e-52b7-42e0-956c-1aa0f5a6f81b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install aiohttp>=3.8.0 aiofiles>=22.1.0 httpx>=0.24.0 selectolax>=0.3.0 nbformat>=5.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ca0a63-b312-4510-97d1-4ee790d624cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"scraper_name\", \"century21\")\n",
    "dbutils.widgets.text(\"process_id\", \"0Z\")\n",
    "\n",
    "scraper_name = dbutils.widgets.get(\"scraper_name\")\n",
    "input_table_name = f\"realitky.raw.listings_{scraper_name}\"\n",
    "output_table_name = f\"realitky.raw.listing_details_{scraper_name}\"\n",
    "output_images_table_name = f\"realitky.raw.listing_images_{scraper_name}\"\n",
    "\n",
    "process_id = dbutils.widgets.get(\"process_id\")\n",
    "insert_mode = \"append\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70fa0e0-7a82-46f0-996d-155ae802a17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Load data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669a8e3c-2a2c-462b-b7a4-2d991ba5fb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT listing_id, listing_url FROM {input_table_name} WHERE parsed = false AND del_flag = false\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf1eef4-947f-4041-b7af-0aa57cf80bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Prepare parsing script\n",
    "Contains:\n",
    "- details (dictionary) + listing_hash\n",
    "- images (list) + images_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d2c526-c7bd-4e2e-9689-9917ddb46575",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Price\":180},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"listing_url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1753264431853}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define async function to fetch property details\n",
    "import asyncio\n",
    "import httpx\n",
    "import hashlib\n",
    "import re\n",
    "from selectolax.parser import HTMLParser\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from typing import Optional, Dict\n",
    "\n",
    "class Scraper_details:\n",
    "    def extract_details_from_table(self, container, row_sel: str, label_sel: str, value_sel: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract details from table-like HTML structure\"\"\"\n",
    "        details = {}\n",
    "        for row in container.css(row_sel):\n",
    "            label = row.css_first(label_sel)\n",
    "            value = row.css_first(value_sel)\n",
    "            if label and value:\n",
    "                details[label.text(strip=True)] = str(value.text(strip=True))\n",
    "        return details\n",
    "    \n",
    "    def extract_direct_image_url(self, next_image_url: str) -> str:\n",
    "        \"\"\"Extract direct image URL from Next.js image wrapper\"\"\"\n",
    "        try:\n",
    "            parsed_url = urlparse(next_image_url)\n",
    "            if parsed_url.path == '/_next/image' and 'url' in parse_qs(parsed_url.query):\n",
    "                # Extract the actual image URL from the 'url' parameter\n",
    "                direct_url = parse_qs(parsed_url.query)['url'][0]\n",
    "                return unquote(direct_url)\n",
    "            else:\n",
    "                # If it's not a Next.js wrapper, return as is\n",
    "                return next_image_url\n",
    "        except:\n",
    "            return next_image_url\n",
    "        \n",
    "    # Async function\n",
    "    async def fetch_property_details(self, listing_id: str, listing_url: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            # Download HTML content and parse it\n",
    "            url = listing_url\n",
    "            async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:\n",
    "                response = await client.get(url)\n",
    "                response.raise_for_status()\n",
    "                parser = HTMLParser(response.text)\n",
    "\n",
    "            # Define details as dict\n",
    "            details = {\n",
    "                \"listing_id\": str(listing_id),\n",
    "                \"listing_url\": str(listing_url),\n",
    "            }\n",
    "            \n",
    "            # Extract category1, category2, category3 from URL\n",
    "            try:\n",
    "                url_path = re.sub(r\"^https?://[^/]+/\", \"\", listing_url)\n",
    "                path_parts = url_path.split(\"/\")\n",
    "                if len(path_parts) > 1:\n",
    "                    segments = path_parts[1].split(\"-\")\n",
    "                    for i in range(min(4, len(segments))):\n",
    "                        details[f\"category{i+1}\"] = segments[i]\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting categories from URL for {listing_id}: {e}\")\n",
    "\n",
    "            # Extract property name\n",
    "            title_element = parser.css_first(\"h3.font-barlow.font-bold\")\n",
    "            if title_element:\n",
    "                details[\"Property Name\"] = str(title_element.text(strip=True))\n",
    "\n",
    "            # Extract address point with translate=\"no\" attribute\n",
    "            address_point = parser.css_first('p.uppercase.font-barlow.font-medium')\n",
    "            if address_point:\n",
    "                details[\"Address Point\"] = str(address_point.text(strip=True))\n",
    "\n",
    "            # Extract property description\n",
    "            description_container = parser.css_first(\"div.text-white.font-light.whitespace-break-spaces\")\n",
    "            if description_container:\n",
    "                details[\"Property Description\"] = str(description_container.text(strip=True))\n",
    "\n",
    "            # Extract property price and basic details\n",
    "            price_container = parser.css(\"div.bg-primary-300.rounded-b-2xl\")\n",
    "            for price_div in price_container:\n",
    "                p_elements = price_div.css(\"p\")\n",
    "                if len(p_elements) > 0:\n",
    "                    details[\"Price Detail\"] = str(p_elements[0].text(strip=True))\n",
    "                if len(p_elements) > 1:\n",
    "                    details[\"Price\"] = str(p_elements[1].text(strip=True))\n",
    "\n",
    "            # Extract property additional details\n",
    "            tables_container = parser.css(\"table.w-full.text-white.border-t-2.border-b-2.border-black\")\n",
    "            for table in tables_container:\n",
    "                details.update(\n",
    "                    self.extract_details_from_table(\n",
    "                        table,\n",
    "                        \"tr\",\n",
    "                        \"th\",\n",
    "                        \"td\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "           # GPS from <script>self.__next_f.push...\n",
    "            gps_found = False\n",
    "            try:\n",
    "                # Get all script tags\n",
    "                script_tags = parser.css(\"script\")\n",
    "                \n",
    "                # First try: look in scripts with both next_f and coordinates\n",
    "                for script in script_tags:\n",
    "                    script_text = script.text()\n",
    "                    if script_text and \"self.__next_f.push\" in script_text and \"coordinates\" in script_text:\n",
    "                        \n",
    "                        # Try specific coordinate patterns\n",
    "                        patterns = [\n",
    "                            r'\"coordinates\":\\{\"latitude\":([\\d.-]+),\"longitude\":([\\d.-]+)\\}',\n",
    "                            r'\"coordinates\":\\s*\\{\\s*\"latitude\":([\\d.-]+),\\s*\"longitude\":([\\d.-]+)\\s*\\}',\n",
    "                            r'\"latitude\":([\\d.-]+),\"longitude\":([\\d.-]+)',\n",
    "                        ]\n",
    "                        \n",
    "                        for pattern in patterns:\n",
    "                            coord_match = re.search(pattern, script_text, re.IGNORECASE)\n",
    "                            if coord_match:\n",
    "                                latitude = coord_match.group(1)\n",
    "                                longitude = coord_match.group(2)\n",
    "                                details[\"GPS coordinates\"] = f\"{latitude},{longitude}\"\n",
    "                                gps_found = True\n",
    "                                break\n",
    "                        \n",
    "                        if gps_found:\n",
    "                            break\n",
    "                \n",
    "                # Second try: look in any script with coordinates (broader search)\n",
    "                if not gps_found:\n",
    "                    for script in script_tags:\n",
    "                        script_text = script.text()\n",
    "                        if script_text and \"coordinates\" in script_text:\n",
    "                            \n",
    "                            # Try all coordinate patterns\n",
    "                            patterns = [\n",
    "                                r'\"coordinates\":\\{\"latitude\":([\\d.-]+),\"longitude\":([\\d.-]+)\\}',\n",
    "                                r'\"coordinates\":\\s*\\{\\s*\"latitude\":([\\d.-]+),\\s*\"longitude\":([\\d.-]+)\\s*\\}',\n",
    "                                r'\"latitude\":([\\d.-]+),\"longitude\":([\\d.-]+)',\n",
    "                                r'\"lat\":([\\d.-]+),\"lng\":([\\d.-]+)',\n",
    "                                r'\"lat\":([\\d.-]+),\"lon\":([\\d.-]+)',\n",
    "                            ]\n",
    "                            \n",
    "                            for pattern in patterns:\n",
    "                                coord_match = re.search(pattern, script_text, re.IGNORECASE)\n",
    "                                if coord_match:\n",
    "                                    latitude = coord_match.group(1)\n",
    "                                    longitude = coord_match.group(2)\n",
    "                                    details[\"GPS coordinates\"] = f\"{latitude},{longitude}\"\n",
    "                                    gps_found = True\n",
    "                                    break\n",
    "                            \n",
    "                            if gps_found:\n",
    "                                break\n",
    "                \n",
    "                # Third try: fallback to numerical pattern in any next_f script\n",
    "                if not gps_found:\n",
    "                    for script in script_tags:\n",
    "                        script_text = script.text()\n",
    "                        if script_text and \"self.__next_f.push\" in script_text:\n",
    "                            \n",
    "                            coord_numbers = re.findall(r'\\b(-?[0-9]{1,2}\\.\\d{4,})\\b.*?\\b(-?1?[0-9]{1,2}\\.\\d{4,})\\b', script_text)\n",
    "                            if coord_numbers:\n",
    "                                for lat_str, lon_str in coord_numbers:\n",
    "                                    lat = float(lat_str)\n",
    "                                    lon = float(lon_str)\n",
    "                                    if -90 <= lat <= 90 and -180 <= lon <= 180 and not (abs(lat) < 0.01 and abs(lon) < 0.01):\n",
    "                                        details[\"GPS coordinates\"] = f\"{lat},{lon}\"\n",
    "                                        gps_found = True\n",
    "                                        break\n",
    "                            \n",
    "                            if gps_found:\n",
    "                                break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting GPS: {e}\")\n",
    "\n",
    "            # Set status\n",
    "            if title_element:\n",
    "                details[\"Status\"] = 'active'\n",
    "            else:\n",
    "                details[\"Status\"] = 'inactive'\n",
    "\n",
    "            # Generate hash\n",
    "            hash_input = {k: v for k, v in details.items()}\n",
    "            listing_hash = hashlib.sha256(str(sorted(hash_input.items())).encode()).hexdigest()\n",
    "            details[\"listing_hash\"] = str(listing_hash)\n",
    "\n",
    "            #######################\n",
    "            ##### -- IMAGES -- ####\n",
    "            #######################\n",
    "            \n",
    "            # Extract property images from Slick carousel\n",
    "            images = []\n",
    "            images_container = parser.css(\"div.slick-slide:not(.slick-cloned)\")\n",
    "            if images_container:\n",
    "                for img_number, slide in enumerate(images_container, 1):\n",
    "                    # Look for div with background-image style\n",
    "                    img_element = slide.css_first(\"div[style*='background-image']\")\n",
    "                    if img_element:\n",
    "                        style = img_element.attributes.get(\"style\", \"\")\n",
    "\n",
    "                        if \"background-image:\" in style:\n",
    "                            # Extract URL from background-image: url(\"...\") or url(...)\n",
    "                            match = re.search(r'background-image:\\s*url\\([\"\\']?(.*?)[\"\\']?\\)', style)\n",
    "                            if match:\n",
    "                                src = match.group(1)                               \n",
    "                                # Create image record\n",
    "                                image_record = {\n",
    "                                    \"listing_id\": str(listing_id),\n",
    "                                    \"img_number\": img_number,\n",
    "                                    \"img_link\": str(src)\n",
    "                                }\n",
    "                                images.append(image_record)\n",
    "            \n",
    "            # If still no images found, create empty record\n",
    "            if not images:\n",
    "                image_record = {\n",
    "                    \"listing_id\": str(listing_id),\n",
    "                    \"img_number\": None,\n",
    "                    \"img_link\": None\n",
    "                }\n",
    "                images.append(image_record)\n",
    "            \n",
    "            # Generate images_hash\n",
    "            image_hash_input = str(sorted([sorted(img.items()) for img in images]))\n",
    "            images_hash = hashlib.sha256(image_hash_input.encode()).hexdigest()\n",
    "            for image_record in images:\n",
    "                image_record[\"images_hash\"] = str(images_hash)\n",
    "            \n",
    "            print(f\"Found {len(images)} images for listing {listing_id}\")\n",
    "            return details, images\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {listing_id}: {e}\")\n",
    "            return None, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004580ce-a64e-44f7-a9ea-caee29cb20f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute parsing\n",
    "- Run the parsing script to extract relevant data from the raw input.\n",
    "- Ensure all required fields are captured and formatted correctly.\n",
    "- Log any errors or missing data for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f7ee98-73b7-467b-8071-819adf917266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Execute the async function\n",
    "print(f\"Found {df.count()} listings\")\n",
    "if df.count() > 0:\n",
    "    scraper = Scraper_details()\n",
    "    results = await asyncio.gather(*[scraper.fetch_property_details(row.listing_id, row.listing_url) for row in df.collect()])\n",
    "    \n",
    "    # Separate details and images\n",
    "    parsed_details = [result[0] for result in results if result[0] is not None] # results = [0, 1]\n",
    "    parsed_images = []\n",
    "    for result in results:\n",
    "        if result[1]:\n",
    "            parsed_images.extend(result[1])\n",
    "    \n",
    "    print(f\"Scraped {len(parsed_details)} listings with {len(parsed_images)} total images\")\n",
    "    \n",
    "    # Create DataFrames\n",
    "    if parsed_details:\n",
    "        df_parsed_details = spark.createDataFrame(parsed_details)\n",
    "    else:\n",
    "        schema = StructType([\n",
    "            StructField(\"listing_id\", StringType(), True),\n",
    "            StructField(\"listing_url\", StringType(), True)\n",
    "        ])\n",
    "        df_parsed_details = spark.createDataFrame([], schema)\n",
    "    \n",
    "    if parsed_images:\n",
    "        df_parsed_images = spark.createDataFrame(parsed_images)\n",
    "    else:\n",
    "        images_schema = StructType([\n",
    "            StructField(\"listing_id\", StringType(), True),\n",
    "            StructField(\"img_number\", StringType(), True),\n",
    "            StructField(\"img_link\", StringType(), True)\n",
    "        ])\n",
    "        df_parsed_images = spark.createDataFrame([], images_schema)\n",
    "\n",
    "print(\"--- DETAILS ---\")\n",
    "display(df_parsed_details)\n",
    "print(\"--- IMAGES ---\")\n",
    "display(df_parsed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2dd550-0c60-4c90-97a8-47126e0f3b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Export data and update stats\n",
    "- From 'clean_column_name.ipynb. get script for cleaning names of columns (eg. no diacritics, lowercase, replace spaces with underscore, etc.)\n",
    "- From 'listing_details_import.ipynb' get script for importing scraped information about property\n",
    "- From 'listing_update.ipynb' get script for updating state of listing_id to be 'parsed = True'\n",
    "- After all get all the listing_ids and set upd_check_date to current date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c0e122-e8d5-4ffc-aee3-ca4f26a4d14b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# import functions\n",
    "%run \"./utils/clean_column_name.ipynb\"\n",
    "%run \"./utils/listing_details_import.ipynb\"\n",
    "%run \"./utils/listings_update.ipynb\"\n",
    "\n",
    "if df_parsed_details.count() > 0:\n",
    "    # Clean column names\n",
    "    df_parsed_details = clean_column_names(df_parsed_details)\n",
    "    df_parsed_images = clean_column_names(df_parsed_images)\n",
    "\n",
    "    # Export scraped data about property\n",
    "    row_count = export_to_table(df_parsed_details, output_table_name, insert_mode, \"listing_hash\")\n",
    "    images_count = export_to_table(df_parsed_images, output_images_table_name, insert_mode, \"images_hash\")\n",
    "\n",
    "    # Update all listing_ids with 'parsed = True'\n",
    "    df_parsed_details.createOrReplaceTempView(\"listing_ids_view\")\n",
    "    update_listings(input_table_name, 'parsed', process_id)\n",
    "\n",
    "    # Update input table with 'upd_check_date = current_date'\n",
    "    spark.sql(f\"\"\"    \n",
    "        UPDATE {input_table_name}\n",
    "            SET upd_check_date = current_date()\n",
    "            WHERE listing_id IN (SELECT listing_id FROM listing_ids_view) AND del_flag = false\n",
    "        \"\"\")\n",
    "else:\n",
    "    row_count = 0\n",
    "\n",
    "# Save row count\n",
    "dbutils.jobs.taskValues.set(\"row_count\", row_count)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7452526412662602,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "scraper_century21_detail",
   "widgets": {
    "process_id": {
     "currentValue": "2B",
     "nuid": "5b7b4260-4c98-489c-a17c-6c1e8ba19cca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0Z",
      "label": null,
      "name": "process_id",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "0Z",
      "label": null,
      "name": "process_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "scraper_name": {
     "currentValue": "century21",
     "nuid": "fe0ab87b-0447-4b9f-94ee-e0449eb8583e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "century21",
      "label": null,
      "name": "scraper_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "century21",
      "label": null,
      "name": "scraper_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
