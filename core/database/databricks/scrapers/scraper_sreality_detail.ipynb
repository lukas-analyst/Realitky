{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70fa0e0-7a82-46f0-996d-155ae802a17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Load data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb1c55e-52b7-42e0-956c-1aa0f5a6f81b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install aiohttp>=3.8.0 aiofiles>=22.1.0 httpx>=0.24.0 selectolax>=0.3.0 nbformat>=5.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ca0a63-b312-4510-97d1-4ee790d624cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scraper_name = dbutils.widgets.get(\"scraper_name\")\n",
    "\n",
    "input_table_name = f\"realitky.raw.listings_{scraper_name}\"\n",
    "output_table_name = f\"realitky.raw.listing_details_{scraper_name}\"\n",
    "\n",
    "process_id = dbutils.widgets.get(\"process_id\")\n",
    "weekly = dbutils.widgets.get(\"weekly\")\n",
    "insert_mode = \"append\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669a8e3c-2a2c-462b-b7a4-2d991ba5fb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT listing_id, listing_url FROM {input_table_name} WHERE parsed = false AND del_flag = false\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf1eef4-947f-4041-b7af-0aa57cf80bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Parse listing details from each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d2c526-c7bd-4e2e-9689-9917ddb46575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import requests\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "if df.count() > 0:\n",
    "    listings_id = [row['listing_id'] for row in df.collect()]\n",
    "    print(f\"Found {df.count()} listings\")\n",
    "    details = []\n",
    "\n",
    "    for listing in listings_id:\n",
    "        try:\n",
    "            BASE_URL = \"https://www.sreality.cz/api/cs/v2/estates/\"\n",
    "            url = BASE_URL + listing\n",
    "            response = requests.get(url, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            listing_detail = response.text\n",
    "            \n",
    "            # Generate hash\n",
    "            hash_input = listing_detail\n",
    "            listing_hash = hashlib.sha256(hash_input.encode()).hexdigest()\n",
    "            \n",
    "            details.append((listing, url, listing_detail, listing_hash))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching details for property ID {listing}: {e}\")\n",
    "\n",
    "        # Small delay to be respectful to the API\n",
    "        time.sleep(0.3)\n",
    "else:\n",
    "    details = []\n",
    "\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"listing_id\", StringType(), True),\n",
    "    StructField(\"listing_url\", StringType(), True),\n",
    "    StructField(\"listing_detail\", StringType(), True),\n",
    "    StructField(\"listing_hash\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Save into Spark DataFrame\n",
    "df_parsed_details = spark.createDataFrame(details, schema=schema)\n",
    "print(f\"Scraped {df_parsed_details.count()} listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c0e122-e8d5-4ffc-aee3-ca4f26a4d14b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# import functions\n",
    "%run \"./utils/clean_column_name.ipynb\"\n",
    "%run \"./utils/listing_details_import.ipynb\"\n",
    "%run \"./utils/listings_update.ipynb\"\n",
    "\n",
    "if df_parsed_details.count() > 0:\n",
    "    # Remove duplicates on listing_id\n",
    "    df_parsed_details = df_parsed_details.dropDuplicates([\"listing_id\"])\n",
    "    \n",
    "    # Clean column names\n",
    "    df_parsed_details = clean_column_names(df_parsed_details)\n",
    "\n",
    "    # Export scraped data about property\n",
    "    row_count = export_to_table(df_parsed_details, output_table_name, insert_mode)\n",
    "\n",
    "    # Update all listing_ids with 'parsed = True'\n",
    "    df_parsed_details.createOrReplaceTempView(\"listing_ids_view\")\n",
    "    update_listings(input_table_name, 'parsed', process_id)\n",
    "    # Update input table with 'upd_check_date = current_date'\n",
    "    spark.sql(f\"\"\"    \n",
    "        UPDATE {input_table_name}\n",
    "            SET upd_check_date = current_date()\n",
    "            WHERE listing_id IN (SELECT listing_id FROM listing_ids_view) AND del_flag = false\n",
    "        \"\"\")\n",
    "else:\n",
    "    row_count = 0\n",
    "\n",
    "# Save row count\n",
    "dbutils.jobs.taskValues.set(\"row_count\", row_count)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7452526412662602,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "scraper_sreality_detail",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
