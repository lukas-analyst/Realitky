{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b029187",
   "metadata": {},
   "source": [
    "# POI Enrichment for Properties\n",
    "\n",
    "This notebook enriches property data with Points of Interest (POI) from various categories using Geoapify API.\n",
    "\n",
    "## Process:\n",
    "1. Load properties with GPS coordinates\n",
    "2. For each property, find nearby POI in multiple categories\n",
    "3. Calculate distances and rankings\n",
    "4. Store results in `realitky.cleaned.property_poi` table\n",
    "\n",
    "## Categories:\n",
    "- Transport: Bus, Tram, Metro, Train\n",
    "- Amenities: Restaurants, Shops, Schools, Healthcare, Sports\n",
    "- Infrastructure: Industry, Airport, Power Plants, Highways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from typing import List, Dict, Optional\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"POI_Enrichment\").getOrCreate()\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration widgets\n",
    "dbutils.widgets.text(\"api_key\", \"your_geoapify_api_key\", \"Geoapify API Key\")\n",
    "dbutils.widgets.text(\"batch_size\", \"10\", \"Batch Size\")\n",
    "dbutils.widgets.text(\"process_id\", \"POI_001\", \"Process ID\")\n",
    "dbutils.widgets.text(\"radius_meters\", \"5000\", \"Search Radius in Meters\")\n",
    "dbutils.widgets.dropdown(\"test_mode\", \"true\", [\"true\", \"false\"], \"Test Mode (limit to 50 records)\")\n",
    "\n",
    "# Get widget values\n",
    "api_key = dbutils.widgets.get(\"api_key\")\n",
    "batch_size = int(dbutils.widgets.get(\"batch_size\"))\n",
    "process_id = dbutils.widgets.get(\"process_id\")\n",
    "radius_meters = int(dbutils.widgets.get(\"radius_meters\"))\n",
    "test_mode = dbutils.widgets.get(\"test_mode\").lower() == \"true\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- API Key: {'*' * (len(api_key) - 4) + api_key[-4:] if len(api_key) > 4 else 'NOT_SET'}\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Process ID: {process_id}\")\n",
    "print(f\"- Search radius: {radius_meters}m\")\n",
    "print(f\"- Test mode: {test_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POI Category mapping to Geoapify categories\n",
    "POI_CATEGORY_MAPPING = {\n",
    "    'transport_bus': ['public_transport.bus'],\n",
    "    'transport_tram': ['public_transport.tram'],\n",
    "    'transport_metro': ['public_transport.subway'],\n",
    "    'transport_train': ['public_transport.railway'],\n",
    "    'restaurant': ['catering.restaurant', 'catering.fast_food', 'catering.cafe'],\n",
    "    'shop': ['commercial.shopping_mall', 'commercial.supermarket', 'commercial.marketplace'],\n",
    "    'school': ['education.school', 'education.university', 'education.nursery'],\n",
    "    'healthcare': ['healthcare.hospital', 'healthcare.clinic', 'healthcare.pharmacy'],\n",
    "    'sports': ['sport.fitness', 'sport.stadium', 'activity.sport_club'],\n",
    "    'industry': ['industrial.factory', 'industrial.warehouse'],\n",
    "    'airport': ['public_transport.airport'],\n",
    "    'power_plant': ['industrial.power'],\n",
    "    'highway': ['highway.motorway', 'highway.trunk']\n",
    "}\n",
    "\n",
    "print(\"POI Category Mapping:\")\n",
    "for category, mappings in POI_CATEGORY_MAPPING.items():\n",
    "    print(f\"  {category}: {mappings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POI Enricher Class Definition\n",
    "class POIEnricher:\n",
    "    def __init__(self, api_key: str, radius_meters: int = 5000):\n",
    "        self.api_key = api_key\n",
    "        self.radius_meters = radius_meters\n",
    "        self.base_url = 'https://api.geoapify.com/v2/places'\n",
    "        \n",
    "        # POI Category mapping to Geoapify categories - moved inside class\n",
    "        self.poi_category_mapping = {\n",
    "            'transport_bus': ['public_transport.bus'],\n",
    "            'transport_tram': ['public_transport.tram'],\n",
    "            'transport_metro': ['public_transport.subway'],\n",
    "            'transport_train': ['public_transport.train_station'],\n",
    "            'restaurant': ['catering.restaurant', 'catering.fast_food', 'catering.cafe'],\n",
    "            'shop': ['commercial.shopping_mall', 'commercial.supermarket', 'commercial.marketplace'],\n",
    "            'school': ['education.school', 'education.university', 'education.nursery'],\n",
    "            'healthcare': ['healthcare.hospital', 'healthcare.clinic', 'healthcare.pharmacy'],\n",
    "            'sports': ['sport.fitness', 'sport.stadium', 'activity.sport_club'],\n",
    "            'industry': ['industrial.factory', 'industrial.warehouse'],\n",
    "            'airport': ['public_transport.airport'],\n",
    "            'power_plant': ['industrial.power'],\n",
    "            'highway': ['highway.motorway', 'highway.trunk']\n",
    "        }\n",
    "        \n",
    "    def get_poi_for_property(self, property_id: str, lat: float, lng: float, category_id: str) -> List[Dict]:\n",
    "        \"\"\"Get POI for a single property and category\"\"\"\n",
    "        try:\n",
    "            # Get category mapping\n",
    "            categories = self.poi_category_mapping.get(category_id, [category_id])\n",
    "            \n",
    "            # Get max results for this category\n",
    "            max_results = self._get_max_results_for_category(category_id)\n",
    "            \n",
    "            all_pois = []\n",
    "            \n",
    "            for category in categories:\n",
    "                params = {\n",
    "                    'categories': category,\n",
    "                    'filter': f'circle:{lng},{lat},{self.radius_meters}',\n",
    "                    'limit': max_results,\n",
    "                    'apiKey': self.api_key\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(self.base_url, params=params, timeout=30)\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    data = response.json()\n",
    "                    \n",
    "                    if 'features' in data:\n",
    "                        for idx, feature in enumerate(data['features']):\n",
    "                            poi_record = self._parse_poi_feature(\n",
    "                                feature, property_id, category_id, lat, lng, idx + 1\n",
    "                            )\n",
    "                            if poi_record:\n",
    "                                all_pois.append(poi_record)\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "                except requests.exceptions.HTTPError as e:\n",
    "                    print(f\"HTTP Error for category {category}: {e}\")\n",
    "                    if e.response.status_code == 400:\n",
    "                        print(f\"Bad request - possibly invalid category: {category}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error for category {category}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "            return all_pois\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching POI for property {property_id}, category {category_id}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_max_results_for_category(self, category_id: str) -> int:\n",
    "        \"\"\"Get maximum results based on category\"\"\"\n",
    "        limits = {\n",
    "            'transport_bus': 10,\n",
    "            'transport_tram': 10,\n",
    "            'transport_metro': 5,\n",
    "            'transport_train': 3,\n",
    "            'restaurant': 5,\n",
    "            'shop': 5,\n",
    "            'school': 5,\n",
    "            'healthcare': 5,\n",
    "            'sports': 10,\n",
    "            'industry': 5,\n",
    "            'airport': 1,\n",
    "            'power_plant': 1,\n",
    "            'highway': 2\n",
    "        }\n",
    "        return limits.get(category_id, 5)\n",
    "    \n",
    "    def _parse_poi_feature(self, feature: Dict, property_id: str, category_id: str, \n",
    "                          property_lat: float, property_lng: float, rank: int) -> Optional[Dict]:\n",
    "        \"\"\"Parse POI feature from API response\"\"\"\n",
    "        try:\n",
    "            props = feature.get('properties', {})\n",
    "            geometry = feature.get('geometry', {})\n",
    "            coordinates = geometry.get('coordinates', [])\n",
    "            \n",
    "            if len(coordinates) < 2:\n",
    "                print(f\"Invalid coordinates for property {property_id}, category {category_id}\")\n",
    "                return None\n",
    "                \n",
    "            poi_lng, poi_lat = coordinates[0], coordinates[1]\n",
    "            \n",
    "            # Convert coordinates to float to handle Decimal types\n",
    "            poi_lng = float(poi_lng)\n",
    "            poi_lat = float(poi_lat)\n",
    "            property_lat = float(property_lat)\n",
    "            property_lng = float(property_lng)\n",
    "            \n",
    "            # Calculate distance\n",
    "            try:\n",
    "                distance_km = self._calculate_distance(\n",
    "                    property_lat, property_lng, poi_lat, poi_lng\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating distance for property {property_id}: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Generate POI ID\n",
    "            poi_id = f\"{property_id}_{category_id}_{rank}_{int(time.time())}\"\n",
    "            \n",
    "            # Extract attributes\n",
    "            attributes = {}\n",
    "            \n",
    "            # Common attributes\n",
    "            if 'datasource' in props:\n",
    "                attributes['data_source'] = props['datasource'].get('sourcename', 'unknown')\n",
    "            \n",
    "            if 'contact' in props:\n",
    "                contact = props['contact']\n",
    "                if 'phone' in contact:\n",
    "                    attributes['phone'] = str(contact['phone'])\n",
    "                if 'email' in contact:\n",
    "                    attributes['email'] = contact['email']\n",
    "            \n",
    "            if 'website' in props:\n",
    "                attributes['website'] = props['website']\n",
    "                \n",
    "            # Category specific attributes\n",
    "            if category_id.startswith('transport_'):\n",
    "                if 'public_transport' in props:\n",
    "                    attributes.update(props['public_transport'])\n",
    "            elif category_id == 'restaurant':\n",
    "                if 'cuisine' in props:\n",
    "                    attributes['cuisine'] = props['cuisine']\n",
    "                if 'rating' in props:\n",
    "                    attributes['rating'] = str(props['rating'])\n",
    "            \n",
    "            poi_record = {\n",
    "                'poi_id': poi_id,\n",
    "                'property_id': property_id,\n",
    "                'category_id': category_id,\n",
    "                'poi_name': props.get('name', 'Unknown'),\n",
    "                'poi_address': props.get('formatted', ''),\n",
    "                'poi_lat': poi_lat,\n",
    "                'poi_lng': poi_lng,\n",
    "                'distance_km': round(distance_km, 3),\n",
    "                'distance_walking_min': self._estimate_walking_time(distance_km),\n",
    "                'distance_driving_min': self._estimate_driving_time(distance_km),\n",
    "                'rank_in_category': rank,\n",
    "                'poi_attributes': attributes,\n",
    "                'data_source': 'geoapify',\n",
    "                'data_quality_score': 1.0,\n",
    "                'ins_dt': datetime.now(),\n",
    "                'ins_process_id': process_id,\n",
    "                'upd_dt': datetime.now(),\n",
    "                'upd_process_id': process_id,\n",
    "                'del_flag': False\n",
    "            }\n",
    "            \n",
    "            return poi_record\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing POI feature for property {property_id}, category {category_id}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _calculate_distance(self, lat1: float, lng1: float, lat2: float, lng2: float) -> float:\n",
    "        \"\"\"Calculate distance between two points using Haversine formula\"\"\"\n",
    "        import math\n",
    "        \n",
    "        # Convert all inputs to float to handle Decimal types from Spark\n",
    "        lat1 = float(lat1)\n",
    "        lng1 = float(lng1)\n",
    "        lat2 = float(lat2)\n",
    "        lng2 = float(lng2)\n",
    "        \n",
    "        R = 6371  # Earth's radius in kilometers\n",
    "        \n",
    "        lat1_rad = math.radians(lat1)\n",
    "        lat2_rad = math.radians(lat2)\n",
    "        delta_lat = math.radians(lat2 - lat1)\n",
    "        delta_lng = math.radians(lng2 - lng1)\n",
    "        \n",
    "        a = (math.sin(delta_lat / 2) * math.sin(delta_lat / 2) +\n",
    "             math.cos(lat1_rad) * math.cos(lat2_rad) *\n",
    "             math.sin(delta_lng / 2) * math.sin(delta_lng / 2))\n",
    "        \n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        distance = R * c\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def _estimate_walking_time(self, distance_km: float) -> int:\n",
    "        \"\"\"Estimate walking time in minutes (assuming 5 km/h)\"\"\"\n",
    "        return int(distance_km * 12)  # 60 minutes / 5 km/h = 12 minutes per km\n",
    "    \n",
    "    def _estimate_driving_time(self, distance_km: float) -> int:\n",
    "        \"\"\"Estimate driving time in minutes (assuming 30 km/h in city)\"\"\"\n",
    "        return int(distance_km * 2)  # 60 minutes / 30 km/h = 2 minutes per km\n",
    "\n",
    "# Initialize POI enricher\n",
    "enricher = POIEnricher(api_key, radius_meters)\n",
    "print(\"POI Enricher initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load properties with GPS coordinates\n",
    "print(\"Loading properties with GPS coordinates...\")\n",
    "\n",
    "# Base query for properties with GPS coordinates\n",
    "base_query = \"\"\"\n",
    "SELECT \n",
    "    property_id,\n",
    "    address_latitude,\n",
    "    address_longitude\n",
    "FROM realitky.cleaned.properties\n",
    "WHERE address_latitude IS NOT NULL \n",
    "AND address_longitude IS NOT NULL\n",
    "AND address_latitude BETWEEN 48.0 AND 51.0  -- Czech Republic bounds\n",
    "AND address_longitude BETWEEN 12.0 AND 19.0\n",
    "AND del_flag = FALSE\n",
    "\"\"\"\n",
    "\n",
    "# Apply test mode limit if enabled\n",
    "if test_mode:\n",
    "    properties_query = base_query + \" LIMIT 50\"\n",
    "    print(\"🧪 TEST MODE: Limited to 50 properties\")\n",
    "else:\n",
    "    properties_query = base_query\n",
    "    print(\"🚀 PRODUCTION MODE: Processing all properties\")\n",
    "\n",
    "df_properties = spark.sql(properties_query)\n",
    "total_properties = df_properties.count()\n",
    "print(f\"Found {total_properties} properties with valid GPS coordinates\")\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample properties:\")\n",
    "df_properties.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6271b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get POI categories from database\n",
    "print(\"Loading POI categories...\")\n",
    "categories_df = spark.sql(\"SELECT category_code FROM realitky.cleaned.poi_categories ORDER BY category_id\")\n",
    "categories = [row.category_code for row in categories_df.collect()]\n",
    "\n",
    "print(f\"POI Categories to process ({len(categories)}):\")\n",
    "for i, category in enumerate(categories, 1):\n",
    "    print(f\"  {i}. {category}\")\n",
    "\n",
    "# Calculate estimated API calls\n",
    "estimated_calls = total_properties * len(categories)\n",
    "print(f\"\\n📊 Estimated API calls: {estimated_calls:,}\")\n",
    "\n",
    "if estimated_calls > 3000:\n",
    "    print(\"⚠️  WARNING: This exceeds Geoapify free tier limit (3,000 calls/day)\")\n",
    "    print(\"   Consider reducing batch_size or enabling test_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process properties in batches\n",
    "def process_property_batch(property_batch: List, categories: List[str]) -> List[Dict]:\n",
    "    \"\"\"Process a batch of properties for all POI categories\"\"\"\n",
    "    all_pois = []\n",
    "    total_batch_size = len(property_batch)\n",
    "    \n",
    "    for idx, property_data in enumerate(property_batch, 1):\n",
    "        property_id = property_data['property_id']\n",
    "        lat = property_data['address_latitude']\n",
    "        lng = property_data['address_longitude']\n",
    "        \n",
    "        print(f\"Processing property {idx}/{total_batch_size}: {property_id}\")\n",
    "        \n",
    "        for category_idx, category_id in enumerate(categories, 1):\n",
    "            try:\n",
    "                print(f\"  - Category {category_idx}/{len(categories)}: {category_id}\")\n",
    "                pois = enricher.get_poi_for_property(property_id, lat, lng, category_id)\n",
    "                all_pois.extend(pois)\n",
    "                \n",
    "                print(f\"    Found {len(pois)} POI\")\n",
    "                \n",
    "                # Rate limiting between categories\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Error processing {property_id} - {category_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Rate limiting between properties\n",
    "        print(f\"  ✅ Completed property {property_id} ({len([p for p in all_pois if p['property_id'] == property_id])} total POI)\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return all_pois\n",
    "\n",
    "print(\"Processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute POI enrichment\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING POI ENRICHMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to list for processing\n",
    "properties_list = [row.asDict() for row in df_properties.limit(batch_size).collect()]\n",
    "\n",
    "print(f\"Processing {len(properties_list)} properties in current batch...\")\n",
    "print(f\"Each property will be processed for {len(categories)} categories\")\n",
    "\n",
    "# Process batch with timing\n",
    "start_time = time.time()\n",
    "poi_results = process_property_batch(properties_list, categories)\n",
    "end_time = time.time()\n",
    "\n",
    "processing_time = end_time - start_time\n",
    "print(f\"\\n🎯 BATCH PROCESSING COMPLETED\")\n",
    "print(f\"- Properties processed: {len(properties_list)}\")\n",
    "print(f\"- POI records found: {len(poi_results)}\")\n",
    "print(f\"- Processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"- Average time per property: {processing_time/len(properties_list):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0370c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to database\n",
    "if poi_results:\n",
    "    print(\"💾 Saving results to database...\")\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    df_poi_results = spark.createDataFrame(poi_results)\n",
    "    \n",
    "    print(\"Sample POI results:\")\n",
    "    df_poi_results.select(\"property_id\", \"category_id\", \"poi_name\", \"distance_km\", \"rank_in_category\").show(10, truncate=False)\n",
    "    \n",
    "    # Insert into POI table using MERGE for upsert\n",
    "    df_poi_results.createOrReplaceTempView(\"temp_poi_results\")\n",
    "    \n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO realitky.cleaned.property_poi AS target\n",
    "    USING temp_poi_results AS source\n",
    "    ON target.property_id = source.property_id \n",
    "       AND target.category_id = source.category_id \n",
    "       AND target.rank_in_category = source.rank_in_category\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            poi_name = source.poi_name,\n",
    "            poi_address = source.poi_address,\n",
    "            poi_lat = source.poi_lat,\n",
    "            poi_lng = source.poi_lng,\n",
    "            distance_km = source.distance_km,\n",
    "            distance_walking_min = source.distance_walking_min,\n",
    "            distance_driving_min = source.distance_driving_min,\n",
    "            poi_attributes = source.poi_attributes,\n",
    "            data_quality_score = source.data_quality_score,\n",
    "            upd_dt = source.upd_dt,\n",
    "            upd_process_id = source.upd_process_id,\n",
    "            del_flag = source.del_flag\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            poi_id, property_id, category_id, poi_name, poi_address,\n",
    "            poi_lat, poi_lng, distance_km, distance_walking_min, distance_driving_min,\n",
    "            rank_in_category, poi_attributes, data_source, data_quality_score,\n",
    "            ins_dt, ins_process_id, upd_dt, upd_process_id, del_flag\n",
    "        )\n",
    "        VALUES (\n",
    "            source.poi_id, source.property_id, source.category_id, source.poi_name, source.poi_address,\n",
    "            source.poi_lat, source.poi_lng, source.distance_km, source.distance_walking_min, source.distance_driving_min,\n",
    "            source.rank_in_category, source.poi_attributes, source.data_source, source.data_quality_score,\n",
    "            source.ins_dt, source.ins_process_id, source.upd_dt, source.upd_process_id, source.del_flag\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(merge_sql)\n",
    "    \n",
    "    print(f\"✅ Successfully processed {len(poi_results)} POI records using MERGE\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No POI data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaffc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"PROCESSING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# POI Statistics by category\n",
    "stats_query = f\"\"\"\n",
    "SELECT \n",
    "    category_id,\n",
    "    COUNT(*) as poi_count,\n",
    "    COUNT(DISTINCT property_id) as properties_count,\n",
    "    ROUND(AVG(distance_km), 3) as avg_distance_km,\n",
    "    ROUND(MIN(distance_km), 3) as min_distance_km,\n",
    "    ROUND(MAX(distance_km), 3) as max_distance_km\n",
    "FROM realitky.cleaned.property_poi\n",
    "WHERE ins_process_id = '{process_id}'\n",
    "GROUP BY category_id\n",
    "ORDER BY poi_count DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"📊 POI Statistics by Category:\")\n",
    "spark.sql(stats_query).show()\n",
    "\n",
    "# Property coverage statistics\n",
    "coverage_query = f\"\"\"\n",
    "SELECT \n",
    "    p.property_id,\n",
    "    COUNT(DISTINCT poi.category_id) as categories_covered,\n",
    "    COUNT(poi.poi_id) as total_poi_found,\n",
    "    ROUND(AVG(poi.distance_km), 3) as avg_distance_km\n",
    "FROM (\n",
    "    SELECT DISTINCT property_id \n",
    "    FROM realitky.cleaned.property_poi \n",
    "    WHERE ins_process_id = '{process_id}'\n",
    ") p\n",
    "LEFT JOIN realitky.cleaned.property_poi poi ON p.property_id = poi.property_id AND poi.ins_process_id = '{process_id}'\n",
    "GROUP BY p.property_id\n",
    "ORDER BY total_poi_found DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"🏠 Property Coverage (Top 10):\")\n",
    "spark.sql(coverage_query).limit(10).show()\n",
    "\n",
    "# Overall summary\n",
    "summary_query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT property_id) as properties_processed,\n",
    "    COUNT(*) as total_poi_found,\n",
    "    COUNT(DISTINCT category_id) as categories_found,\n",
    "    ROUND(AVG(distance_km), 3) as avg_distance_km,\n",
    "    ROUND(AVG(data_quality_score), 3) as avg_quality_score\n",
    "FROM realitky.cleaned.property_poi\n",
    "WHERE ins_process_id = '{process_id}'\n",
    "\"\"\"\n",
    "\n",
    "print(\"📈 Overall Summary:\")\n",
    "spark.sql(summary_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d49ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality and validation checks\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for missing data\n",
    "missing_data_query = f\"\"\"\n",
    "SELECT \n",
    "    SUM(CASE WHEN poi_name IS NULL OR poi_name = '' THEN 1 ELSE 0 END) as missing_names,\n",
    "    SUM(CASE WHEN poi_address IS NULL OR poi_address = '' THEN 1 ELSE 0 END) as missing_addresses,\n",
    "    SUM(CASE WHEN poi_lat IS NULL THEN 1 ELSE 0 END) as missing_lat,\n",
    "    SUM(CASE WHEN poi_lng IS NULL THEN 1 ELSE 0 END) as missing_lng,\n",
    "    SUM(CASE WHEN distance_km IS NULL OR distance_km < 0 THEN 1 ELSE 0 END) as invalid_distances,\n",
    "    COUNT(*) as total_records\n",
    "FROM realitky.cleaned.property_poi\n",
    "WHERE ins_process_id = '{process_id}'\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 Data Quality Checks:\")\n",
    "spark.sql(missing_data_query).show()\n",
    "\n",
    "# Distance distribution\n",
    "distance_distribution_query = f\"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN distance_km <= 0.5 THEN '0-0.5km'\n",
    "        WHEN distance_km <= 1.0 THEN '0.5-1km'\n",
    "        WHEN distance_km <= 2.0 THEN '1-2km'\n",
    "        WHEN distance_km <= 5.0 THEN '2-5km'\n",
    "        ELSE '5km+'\n",
    "    END as distance_range,\n",
    "    COUNT(*) as poi_count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "FROM realitky.cleaned.property_poi\n",
    "WHERE ins_process_id = '{process_id}'\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN distance_km <= 0.5 THEN '0-0.5km'\n",
    "        WHEN distance_km <= 1.0 THEN '0.5-1km'\n",
    "        WHEN distance_km <= 2.0 THEN '1-2km'\n",
    "        WHEN distance_km <= 5.0 THEN '2-5km'\n",
    "        ELSE '5km+'\n",
    "    END\n",
    "ORDER BY \n",
    "    CASE \n",
    "        WHEN distance_range = '0-0.5km' THEN 1\n",
    "        WHEN distance_range = '0.5-1km' THEN 2\n",
    "        WHEN distance_range = '1-2km' THEN 3\n",
    "        WHEN distance_range = '2-5km' THEN 4\n",
    "        ELSE 5\n",
    "    END\n",
    "\"\"\"\n",
    "\n",
    "print(\"📏 Distance Distribution:\")\n",
    "spark.sql(distance_distribution_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49314af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and next steps\n",
    "processing_summary = f\"\"\"\n",
    "=== POI ENRICHMENT COMPLETED ===\n",
    "\n",
    "✅ **Results:**\n",
    "- Properties processed: {len(properties_list)}\n",
    "- POI records created: {len(poi_results) if poi_results else 0}\n",
    "- Process ID: {process_id}\n",
    "- Test mode: {test_mode}\n",
    "\n",
    "🔄 **Next Steps:**\n",
    "1. Update property_poi_summary table\n",
    "2. Calculate location scores\n",
    "3. Verify data quality\n",
    "4. Scale to full dataset (if test_mode was used)\n",
    "\n",
    "⚠️ **Important Notes:**\n",
    "- This {'was a limited test run' if test_mode else 'processed the current batch'}\n",
    "- For full dataset, implement proper batching\n",
    "- Monitor API rate limits\n",
    "- Consider caching results for nearby properties\n",
    "\n",
    "💰 **API Usage:**\n",
    "- Estimated calls made: {len(properties_list) * len(categories) if poi_results else 0}\n",
    "- Geoapify free tier: 3,000 requests/day\n",
    "- Consider upgrading for production use\n",
    "\n",
    "🎯 **Performance:**\n",
    "- Average processing time per property: {(end_time - start_time)/len(properties_list):.2f} seconds\n",
    "- Total batch processing time: {end_time - start_time:.2f} seconds\n",
    "\n",
    "📋 **Data Quality:**\n",
    "- All POI records include distance calculations\n",
    "- Coordinates validated within Czech Republic bounds\n",
    "- API responses parsed with error handling\n",
    "\"\"\"\n",
    "\n",
    "print(processing_summary)\n",
    "\n",
    "# Save processing log\n",
    "log_data = {\n",
    "    'process_id': process_id,\n",
    "    'processing_date': datetime.now(),\n",
    "    'properties_processed': len(properties_list),\n",
    "    'poi_records_created': len(poi_results) if poi_results else 0,\n",
    "    'categories_processed': len(categories),\n",
    "    'test_mode': test_mode,\n",
    "    'processing_time_seconds': end_time - start_time,\n",
    "    'api_calls_estimated': len(properties_list) * len(categories),\n",
    "    'batch_size': batch_size,\n",
    "    'radius_meters': radius_meters\n",
    "}\n",
    "\n",
    "print(\"📝 Processing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
