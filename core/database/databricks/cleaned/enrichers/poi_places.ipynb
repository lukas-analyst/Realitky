{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f0df81",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n",
    "\n",
    "This section sets up interactive widgets for configuring the enrichment process. \n",
    "\n",
    "- Geoapify API key, \n",
    "- POI category (included as an variable from the INPUT parameter), \n",
    "- process ID ({{Job.Run_id}}), \n",
    "- number of properties to process (limited by 'test_mode' to 5, else 50 (beacuse API limits))\n",
    "- test mode (from the Config task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration widgets\n",
    "dbutils.widgets.text(\"api_key\", \"your_geoapify_api_key\", \"Geoapify API Key\")\n",
    "dbutils.widgets.text(\"category_key\", \"1\", \"category_key\")\n",
    "dbutils.widgets.text(\"process_id\", \"POI_001\", \"Process ID\")\n",
    "dbutils.widgets.text(\"max_properties\", \"2\", \"Number of Records\")\n",
    "dbutils.widgets.dropdown(\"test_mode\", \"true\", [\"true\", \"false\"], \"Test Mode (limit to 50 records)\")\n",
    "\n",
    "\n",
    "# Get widget values\n",
    "api_key = dbutils.widgets.get(\"api_key\")\n",
    "category_key = int(dbutils.widgets.get(\"category_key\"))\n",
    "process_id = dbutils.widgets.get(\"process_id\")\n",
    "max_properties = int(dbutils.widgets.get(\"max_properties\"))\n",
    "\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- API Key: {'*' * (len(api_key) - 4) + api_key[-4:] if len(api_key) > 4 else 'NOT_SET'}\")\n",
    "print(f\"- Categeory Key: {category_key}\")\n",
    "print(f\"- Process ID: {process_id}\")\n",
    "print(f\"- Number of properties: {max_properties}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9a06a",
   "metadata": {},
   "source": [
    "### Select POI Category and Properties to Enrich\n",
    "\n",
    "This section retrieves the POI category code and maximum number of POIs to fetch for the selected category.\n",
    "It then selects the properties that need to be enriched with POI data, based on the current configuration. \n",
    "\n",
    "The selection ensures only valid properties (with latitude and longitude) are included, and limits the number of records for efficient processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get POI Category\n",
    "row = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "      category_code, \n",
    "      max_results \n",
    "    FROM realitky.cleaned.poi_category \n",
    "    WHERE \n",
    "      category_key = {category_key} \n",
    "      AND del_flag = FALSE\n",
    "\"\"\").first()\n",
    "\n",
    "category_code = row['category_code']\n",
    "max_results = row['max_results']\n",
    "\n",
    "print(f\"Category: {category_code}\")\n",
    "print(f\"Max results: {max_results}\")\n",
    "\n",
    "# Get properties to enrich\n",
    "df_properties_to_be_enriched = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "      property.property_id, \n",
    "      property.address_latitude, \n",
    "      property.address_longitude\n",
    "    FROM realitky.cleaned.property AS property\n",
    "    FULL OUTER JOIN realitky.stats.property_stats\n",
    "      ON property_stats.property_id = property.property_id \n",
    "     AND property_stats.src_web = property.src_web\n",
    "     AND property_stats.poi_places_check <> TRUE\n",
    "     AND property_stats.del_flag = FALSE\n",
    "    WHERE \n",
    "      property.property_type_id IN (1, 2, 7, 15) -- Byt, Dům, Chata, Rekreační objekt\n",
    "      AND property.address_latitude > 0\n",
    "      AND property.address_longitude > 0\n",
    "      AND property.del_flag = FALSE\n",
    "    ORDER BY\n",
    "      property_stats.ins_dt DESC,\n",
    "      property_stats.upd_dt    \n",
    "    LIMIT {max_properties}\n",
    "\"\"\")\n",
    "display(df_properties_to_be_enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faf984",
   "metadata": {},
   "source": [
    "### Download POI data from Geoapify API for each property\n",
    "\n",
    "This block iterates over properties to be enriched, calls the Geoapify Places API, and collects the raw JSON responses for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from pyspark.sql.types import StructType\n",
    "from datetime import datetime\n",
    "\n",
    "BASE_URL = 'https://api.geoapify.com/v2/places'\n",
    "\n",
    "all_pois = []\n",
    "\n",
    "print(f\"Getting POIs for {df_properties_to_be_enriched.count()} properties\")\n",
    "\n",
    "for idx, row in enumerate(df_properties_to_be_enriched.collect(), 1):\n",
    "    property_id = row['property_id']\n",
    "    address_latitude = row['address_latitude']\n",
    "    address_longitude = row['address_longitude']\n",
    "    params = {\n",
    "        'categories': category_code,\n",
    "        'bias': f'proximity:{address_longitude},{address_latitude}',\n",
    "        'limit': max_results,\n",
    "        'apiKey': api_key\n",
    "    }\n",
    "    print(f\"Requesting POI category=\\\"{category_code}\\\" for property_id='{property_id}' at ({address_latitude}, {address_longitude})\")\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        poi_data = response.json()\n",
    "        all_pois.append({\n",
    "            \"property_id\": property_id,\n",
    "            \"category_key\": category_key,\n",
    "            \"poi_raw_response\": json.dumps(poi_data, ensure_ascii=False) \n",
    "        })\n",
    "        print(f\"Success for property_id={property_id}, found {len(poi_data.get('features', []))} POIs.\")\n",
    "        time.sleep(0.1)\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        print(f\"TIMEOUT for property {property_id}: {e}\\nParams: {params}\")\n",
    "        continue\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTPError for property {property_id}: {e}\\nStatus: {getattr(e.response, 'status_code', None)}\\nParams: {params}\")\n",
    "        if e.response is not None and e.response.status_code == 400:\n",
    "            print(f\"Bad request for property {property_id}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error for property {property_id}: {e}\\nParams: {params}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Finished POI download. Total successful: {len(all_pois)}\")\n",
    "\n",
    "# If df_all_pois is empty, leave df_all_pois empty\n",
    "if all_pois:\n",
    "    df_all_pois = spark.createDataFrame(all_pois)\n",
    "    display(df_all_pois)\n",
    "else:\n",
    "    df_all_pois = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd709b8",
   "metadata": {},
   "source": [
    "### POI Data Cleaning and Transformation\n",
    "This step processes the raw POI (Point of Interest) data by:\n",
    "- Inferring the JSON schema from a sample row.\n",
    "- Parsing the `poi_raw_response` JSON and exploding the features array.\n",
    "- Extracting relevant POI fields such as name, address, coordinates, and data source.\n",
    "- Generating additional metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.functions import from_json, col, explode_outer, schema_of_json, element_at, current_timestamp, lit, concat, udf, trim\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "if df_all_pois: \n",
    "    # UDF - extract empty field\n",
    "    def extract_empty_field(json_str, key):\n",
    "        try:\n",
    "            obj = json.loads(json_str)\n",
    "            features = obj.get('features', [])\n",
    "            for feature in features:\n",
    "                value = feature.get('properties', {}).get(key)\n",
    "                if value is not None:\n",
    "                    return value\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "    extract_name_udf = udf(lambda x: extract_empty_field(x, 'name'), StringType())\n",
    "    extract_address_line1_udf = udf(lambda x: extract_empty_field(x, 'address_line1'), StringType())\n",
    "    extract_address_line2_udf = udf(lambda x: extract_empty_field(x, 'address_line2'), StringType())\n",
    "\n",
    "\n",
    "    # Use the raw response for schema inference\n",
    "    sample_row = df_all_pois.select(\"poi_raw_response\").filter(col(\"poi_raw_response\").isNotNull()).first()\n",
    "    if sample_row is not None:\n",
    "        sample_json = sample_row[\"poi_raw_response\"]\n",
    "        inferred_schema = schema_of_json(sample_json)\n",
    "    else:\n",
    "        inferred_schema = schema_of_json('{}')\n",
    "\n",
    "\n",
    "    # Parse JSON and explode features\n",
    "    df = df_all_pois.withColumn(\"json\", from_json(col(\"poi_raw_response\"), inferred_schema))\n",
    "    df = df.withColumn(\"feature\", explode_outer(col(\"json.features\")))\n",
    "\n",
    "\n",
    "    # Extract fields using UDFs\n",
    "    df = df.withColumn(\"poi_name\", extract_name_udf(col(\"poi_raw_response\")))\n",
    "    df = df.withColumn(\"poi_address1\", extract_address_line1_udf(col(\"poi_raw_response\")))\n",
    "    df = df.withColumn(\"poi_address2\", extract_address_line2_udf(col(\"poi_raw_response\")))\n",
    "\n",
    "\n",
    "    # Add metadata columns\n",
    "    df = df.withColumn(\"ins_dt\", current_timestamp()) \\\n",
    "        .withColumn(\"ins_process_id\", lit(process_id)) \\\n",
    "        .withColumn(\"upd_dt\", current_timestamp()) \\\n",
    "        .withColumn(\"upd_process_id\", lit(process_id)) \\\n",
    "        .withColumn(\"del_flag\", lit(False))\n",
    "\n",
    "\n",
    "    # Select and cast final columns\n",
    "    df_final = df.select(\n",
    "        col(\"category_key\"),\n",
    "        col(\"property_id\"),\n",
    "        col(\"feature.properties\").cast(StringType()).alias(\"poi_attributes\"),\n",
    "        element_at(col(\"feature.geometry.coordinates\"), 2).alias(\"poi_latitude\"),\n",
    "        element_at(col(\"feature.geometry.coordinates\"), 1).alias(\"poi_longitude\"),\n",
    "        col(\"poi_name\"),\n",
    "        col(\"feature.properties.place_id\").alias(\"poi_id\"),\n",
    "        col(\"feature.properties.distance\").alias(\"poi_distance_m\"),\n",
    "        col(\"poi_address1\"),\n",
    "        col(\"poi_address2\"),\n",
    "        col(\"feature.properties.datasource.sourcename\").alias(\"data_source\"),\n",
    "        concat(\n",
    "            lit(\"https://www.openstreetmap.org/#map=19/\"),\n",
    "            element_at(col(\"feature.geometry.coordinates\"), 2), lit(\"/\"),\n",
    "            element_at(col(\"feature.geometry.coordinates\"), 1)\n",
    "        ).alias(\"poi_url\"),\n",
    "        col(\"ins_dt\"),\n",
    "        col(\"ins_process_id\"),\n",
    "        col(\"upd_dt\"),\n",
    "        col(\"upd_process_id\"),\n",
    "        col(\"del_flag\")\n",
    "    )\n",
    "\n",
    "\n",
    "    # Filter out empty or null POI attributes\n",
    "    df_final = df_final.filter(~((trim(col(\"poi_attributes\")) == \"{}\") | (col(\"poi_attributes\").isNull())))\n",
    "\n",
    "\n",
    "    display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bacabc",
   "metadata": {},
   "source": [
    "\n",
    "### Writing POI to the property_poi table by partitioning on category_key\n",
    "\n",
    "This step saves the processed POI to the `realitky.cleaned.property_poi` table only for the current `category_key`. Thanks to partitioning, it is possible to write concurrently for different categories without conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.createOrReplaceTempView(\"tmp_property_poi\")\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO realitky.cleaned.property_poi AS target\n",
    "    USING tmp_property_poi AS source\n",
    "    ON target.property_id = source.property_id\n",
    "        AND target.category_key = source.category_key\n",
    "        AND target.poi_id = source.poi_id\n",
    "    WHEN MATCHED \n",
    "        AND target.category_key = {category_key} -- partitioning\n",
    "        AND target.poi_attributes <> source.poi_attributes\n",
    "        OR target.del_flag <> source.del_flag\n",
    "    THEN UPDATE SET\n",
    "        target.poi_attributes = source.poi_attributes,\n",
    "        target.poi_latitude = source.poi_latitude,\n",
    "        target.poi_longitude = source.poi_longitude,\n",
    "        target.poi_name = source.poi_name,\n",
    "        target.poi_distance_m = source.poi_distance_m,\n",
    "        target.poi_address1 = source.poi_address1,\n",
    "        target.poi_address2 = source.poi_address2,\n",
    "        target.data_source = source.data_source,\n",
    "        target.poi_url = source.poi_url,\n",
    "        target.upd_dt = source.upd_dt,\n",
    "        target.upd_process_id = source.upd_process_id,\n",
    "        target.del_flag = source.del_flag\n",
    "    WHEN NOT MATCHED \n",
    "    THEN INSERT(\n",
    "        category_key,\n",
    "        property_id,\n",
    "        poi_attributes,\n",
    "        poi_latitude,\n",
    "        poi_longitude,\n",
    "        poi_name,\n",
    "        poi_id,\n",
    "        poi_distance_m,\n",
    "        poi_address1,\n",
    "        poi_address2,\n",
    "        data_source,\n",
    "        poi_url,\n",
    "        ins_dt,\n",
    "        ins_process_id,\n",
    "        upd_dt,\n",
    "        upd_process_id,\n",
    "        del_flag\n",
    "    ) VALUES(\n",
    "        source.category_key,\n",
    "        source.property_id,\n",
    "        source.poi_attributes,\n",
    "        source.poi_latitude,\n",
    "        source.poi_longitude,\n",
    "        source.poi_name,\n",
    "        source.poi_id,\n",
    "        source.poi_distance_m,\n",
    "        source.poi_address1,\n",
    "        source.poi_address2,\n",
    "        source.data_source,\n",
    "        source.poi_url,\n",
    "        source.ins_dt,\n",
    "        source.ins_process_id,\n",
    "        source.upd_dt,\n",
    "        source.upd_process_id,\n",
    "        source.del_flag\n",
    "    )\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
