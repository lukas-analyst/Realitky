-- Databricks notebook source  
-- MAGIC %md
-- MAGIC **mapping name:** scd_addr</br>
-- MAGIC **stereotype:** History Generic(id:1870)</br>
-- MAGIC **generated at:** Jul 22, 2025, 9:52:44 AM</br>
-- MAGIC **template version:** 34</br>
-- MAGIC **description:** scd_addr</br>
-- MAGIC **status:** EMPTY</br>
-- MAGIC **responsible analyst**: EMPTY</br>
-- MAGIC **responsible person:** EMPTY</br>
-- MAGIC
 
-- COMMAND ----------
 
-- MAGIC %python
-- MAGIC from workflows.databricks.python.utilities.logger.execution_logger import ETLStepExecutionLogger
-- MAGIC
-- MAGIC job_id = dbutils.widgets.get("dbx_job_id")
-- MAGIC job_run_id = dbutils.widgets.get("dbx_job_run_id")
-- MAGIC job_task_run_id = dbutils.widgets.get("dbx_task_run_id")
-- MAGIC job_id = dbutils.widgets.get("job_id")
-- MAGIC load_date = dbutils.widgets.get("load_date")
-- MAGIC etl_log_key = dbutils.widgets.get("etl_log_key")
-- MAGIC env = dbutils.widgets.get("env")
-- MAGIC extension = dbutils.widgets.get("extension")
-- MAGIC
-- MAGIC etl_log_step = ETLStepExecutionLogger(spark)
-- MAGIC etl_log_step.execution_state["etl_log_key"] = etl_log_key
-- MAGIC
-- MAGIC print("Execution context:")
-- MAGIC print(f"job_run_id:{job_run_id}")
-- MAGIC print(f"job_task_run_id:{job_task_run_id}")
-- MAGIC print(f"job_id:{job_id}")
-- MAGIC print(f"load_date:{load_date}")
-- MAGIC print(f"etl_log_key:{etl_log_key}")
-- MAGIC print(f"env:{env}")
-- MAGIC print(f"extension:{extension}")
-- MAGIC 
-- MAGIC if extension == "":
-- MAGIC   raise Exception("Extension parameter is required, but was not provided!")
 
-- COMMAND ----------
 
-- define parameter
set var.dt = '${load_date}';
set var.job = '${job_id}.${dbx_job_run_id}.${dbx_task_run_id}';
set var.extension = case when '${extension}' = '' then null else '${extension}' end;
--/ define parameter
-- define constant
set var.ftr_dt = '2999-12-31';
set var.past_dt = '1900-01-01';
set var.xna = 'XNA';
--/ define constant
 
MERGE INTO gdp_base_${env}.pt.addr_h trg USING (
     SELECT
        addr_key,
        ltt,
        lgt,
        addr_name,
        str_name,
        reg_num,
        bloc_in_build,
        flr,
        apmnt_num,
        city,
        zip_code,
        terr_admin_unit_key,
        del_flag,
        src_sys_id,
        src_id,
        cntry_id,
        data_own_id
     FROM  
        gdp_base_${env}.pt.addr
     WHERE
        gdp_base_${env}.pt.addr.src_sys_id = nvl(${var.extension},src_sys_id)
        --  Source Condition
        AND gdp_base_${env}.pt.addr.upd_eff_date = ${var.dt}
        --/ Source Condition
     MINUS
     SELECT
        addr_key,
        ltt,
        lgt,
        addr_name,
        str_name,
        reg_num,
        bloc_in_build,
        flr,
        apmnt_num,
        city,
        zip_code,
        terr_admin_unit_key,
        del_flag,
        src_sys_id,
        src_id,
        cntry_id,
        data_own_id
     FROM
        gdp_base_${env}.pt.addr_h
     WHERE
         gdp_base_${env}.pt.addr_h.valid_to = ${var.ftr_dt}
        AND  gdp_base_${env}.pt.addr_h.src_sys_id = nvl(${var.extension},src_sys_id)
     ) src        
ON
    (
        trg.valid_to = ${var.ftr_dt} and
        trg.valid_from = ${var.dt} and  -- Update those already processed today, otherwise insert changes with a new date.
        trg.src_sys_id = nvl(${var.extension},trg.src_sys_id) and -- src_sys_id is always a key column in this solution
        trg.addr_key = src.addr_key
    )   
      -- Update rows with repeated load of the same day pCurrent_date:
WHEN matched then update set
          trg.upd_proc_id = ${var.job},
          trg.upd_dt = current_timestamp(),
          trg.upd_eff_date = ${var.dt},
          trg.ltt = src.ltt,
          trg.lgt = src.lgt,
          trg.addr_name = src.addr_name,
          trg.str_name = src.str_name,
          trg.reg_num = src.reg_num,
          trg.bloc_in_build = src.bloc_in_build,
          trg.flr = src.flr,
          trg.apmnt_num = src.apmnt_num,
          trg.city = src.city,
          trg.zip_code = src.zip_code,
          trg.terr_admin_unit_key = src.terr_admin_unit_key,
          trg.del_flag = src.del_flag,
          trg.src_sys_id = src.src_sys_id,
          trg.src_id = src.src_id,
          trg.cntry_id = src.cntry_id,
          trg.data_own_id = src.data_own_id
  -- Inset new version rows:
WHEN not matched then insert
     (
        addr_key,
        ltt,
        lgt,
        addr_name,
        str_name,
        reg_num,
        bloc_in_build,
        flr,
        apmnt_num,
        city,
        zip_code,
        terr_admin_unit_key,
        src_sys_id,
        src_id,
        cntry_id,
        data_own_id,
        --Audit
        ins_proc_id,
        del_flag,
        upd_proc_id,
        upd_eff_date,
        upd_dt,
        ins_dt,
        --History,
        valid_from,
        valid_to
     ) values (
        src.addr_key,
        src.ltt,
        src.lgt,
        src.addr_name,
        src.str_name,
        src.reg_num,
        src.bloc_in_build,
        src.flr,
        src.apmnt_num,
        src.city,
        src.zip_code,
        src.terr_admin_unit_key,
        src.src_sys_id,
        src.src_id,
        src.cntry_id,
        src.data_own_id,
        --Audit
        ${var.job}, -- INS_PROC_ID
        src.del_flag,
        ${var.job}, -- UPD_PROC_ID
        ${var.dt}, -- UPD_EFF_DT
        current_timestamp(), -- UPD_DT
        current_timestamp(), -- INS_DT
        --History
        ${var.dt}, -- VALID_FROM 
        ${var.ftr_dt}      -- VALID_TO
     );
 
-- COMMAND ----------
 
-- MAGIC %python
-- MAGIC import time
-- MAGIC etl_log_step({"log_step_name": "New history inserted", "log_text": "Finished inserting new history records", "log_step_ord": 1})
 
-- COMMAND ----------
 
-- Close validity of older version rows
MERGE into gdp_base_${env}.pt.addr_h TRG using (
    SELECT     
      addr_h.addr_key,
      addr_h.src_sys_id,
      addr_h.valid_from,    
      addr_h.valid_to  
    FROM 
        (
        SELECT         
           addr_key,
           src_sys_id,
           valid_from,         
           valid_to as valid_to_old,
           LEAD(valid_from-1,1) OVER (PARTITION BY  addr_key  ORDER BY valid_from ASC) AS valid_to        
        FROM
           gdp_base_${env}.pt.addr_h
        WHERE
           gdp_base_${env}.pt.addr_h.valid_to = ${var.ftr_dt}
           AND gdp_base_${env}.pt.addr_h.src_sys_id = nvl(${var.extension},src_sys_id)  
        ) addr_h
    WHERE    
      addr_h.valid_from < ${var.dt} 
      AND addr_h.valid_to <> addr_h.valid_to_old
  ) src
ON
  (     
       trg.src_sys_id = nvl(${var.extension},trg.src_sys_id) AND       
       trg.valid_from = src.valid_from AND
       trg.addr_key = src.addr_key
  )     
WHEN MATCHED AND trg.valid_to <> src.valid_to THEN UPDATE SET
       trg.valid_to = src.valid_to,
       trg.upd_proc_id = ${var.job},
       trg.upd_dt = current_timestamp()
  ;
 
-- COMMAND ----------
 
-- MAGIC %python
-- MAGIC import time
-- MAGIC etl_log_step({"log_step_name": "Old records validity closed", "log_text": "Finished closing validity for old history records", "log_step_ord": 2})
 
-- COMMAND ----------
 
-- MAGIC %python
-- MAGIC from delta.tables import *
-- MAGIC dt = DeltaTable.forName(spark, f"gdp_base_{env}.pt.addr_h")
-- MAGIC df = dt.toDF().where(f"upd_proc_id = '{job_id}.{job_run_id}.{job_task_run_id}'")
-- MAGIC processed_rows = df.count()
-- MAGIC print("Processed rows count:" + str(processed_rows))
-- MAGIC
-- MAGIC etl_log_step({"log_step_name": "second mapping insert", "log_text": f"total inserted rows: {str(processed_rows)}", "log_step_ord": 2})
-- MAGIC
-- MAGIC dbutils.notebook.exit(processed_rows)